## Многослойный Перцептрон (MLP)

**Многослойный перцептрон (MLP)** — это базовая модель нейронной сети, состоящая из последовательности линейных слоев (полносвязных слоев), разделенных активационными функциями. Эта модель может использоваться для решения задач классификации, регрессии и других задач машинного обучения.

### Инициализация

```
MLP(input_size: int, mlp_hidden_config: List[Tuple[int, Activation]], num_classes: int = None, last_activation: Activation = None)
```

- input_size (int): Размер входного вектора.
- mlp_hidden_config (List[Tuple[int, Activation]]): Список, определяющий конфигурацию скрытых слоев. Каждый элемент списка представляет собой кортеж, содержащий количество нейронов в слое и активационную функцию.
- num_classes (int, optional): Количество классов на выходе. По умолчанию равно 1.
- last_activation (Activation, optional): Активационная функция для последнего слоя. Если None, активация не применяется.

### Методы

- forward(x): Прямой проход через сеть.
    - x (np.array): Входные данные.
    - Возвращает: Выходные данные сети.

- backward(grad_output): 
  - Обратный проход через сеть.
  - grad_output (np.array): Градиент потерь по выходу сети.
  - Возвращает: Градиент потерь по входу сети.

- zero_grad(): Обнуляет градиенты всех параметров сети.

- set_parameters(parameters): Устанавливает параметры сети.
  - parameters (dict): Словарь параметров.
- predict(x) Генерирует предсказание
- load(string) Загружает текущую модель по указанному пути
- save(string) Сохраняет модель по указанному пути

### Структура сети

Структура сети может быть получена путем вызова метода __repr__() экземпляра класса. Это позволит вам увидеть конфигурацию слоев, включая тип слоя, количество входных и выходных фич.

### Пример использования


```
from modules.activation import ReLU
from your_module import MLP

# Создание экземпляра MLP с 1 скрытым слоем размером 100 и ReLU активацией
model = MLP(input_size=784, mlp_hidden_config=[(100, ReLU)], num_classes=10)

# Прямой проход
output = model.forward(input_data)

# Обратный проход и обновление параметров сети можно реализовать с использованием выбранного метода оптимизации
```

## Trainer

### Инициализация Trainer
Чтобы начать работу с классом Trainer, необходимо инициализировать его с определенными параметрами:

```
trainer = Trainer(
    model=MLP(...),  # Экземпляр вашей модели MLP
    loss_fn=Loss(...),  # Экземпляр функции потерь
    optimizer=Optimizer(...),  # Экземпляр оптимизатора
    dataset=Dataset(...),  # Экземпляр датасета
    test_split_ratio=0.2,  # Доля датасета для тестирования
    val_split_ratio=0.1,  # Доля датасета для валидации
    batch_size=32,  # Размер батча
    shuffle=True,  # Перемешивание датасета
    mode=Mode.CLASSIFICATION,  # Режим работы: классификация или регрессия
    feature_transformers=[...],  # Преобразователи признаков
    target_transformers=[...]  # Преобразователи целевых переменных
)
```

### Обучение модели
Для запуска процесса обучения используйте метод train, указав необходимое количество эпох и частоту валидации:

```
trainer.train(epochs=50, validation_freq=5, early_stopping=EarlyStopping(...), show_chart=True)
```

- epochs: количество эпох обучения.
- validation_freq: частота валидации (каждые N эпох).
- early_stopping: параметр ранней остановки для предотвращения переобучения.
- show_chart: отображение графиков потерь во время обучения.


### Валидация и тестирование
Метод train автоматически проведет валидацию модели с заданной частотой. Для тестирования модели на отложенной выборке используйте метод test:


```
trainer.test()
```